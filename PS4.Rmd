---
title: "PS4"
author: "Justin Post"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

## Read in some data

Let's start by reading in some data from a URL. This is a `.csv` file so it is a comma delimited file. Opening it, we saw that there is a header row and some missing values. But it seems like it should read in just fine with `read_csv()`
```{r}
library(tidyverse)
sleep_data <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/sleep_data.csv")
```

The file at <https://www4.stat.ncsu.edu/~online/datasets/sleep_data_info.txt> gives the reference for where this data comes from. Let's recreate the first plot (almost) that we see in that paper.

```{r}
#create a plot and define parts of the plot
ggplot(sleep_data, aes(x = TotalSleepTime, 
                       y = term_gpa,
                       color = cohort)) +
  geom_point() + #add a scatter plot layer
  geom_smooth(method = "lm", se = FALSE) + #add SLR fits
  xlab("Total Sleep Time") + #change x label
  ylab("Spring Term GPA") + #change y label
  ggtitle("Scatter Plot with SLR Fits") #change title
```


## Google's Big Query

Lastly, let's connect to Big Query. This is an cloud resource from google. They have some example large datasets up there and you can also put your data up there if you want. 

There are steps we didn't do here which involved setting up a project. Once set up (no credit card entered) you get a project ID that you can use to do some querying!

This code is modified from <https://cloud.google.com/vertex-ai/docs/workbench/user-managed/use-r-bigquery>. Once each R session, you need to run this code in the console so you can log in (unless you take care of authentication in another way).

```{r}
library(bigrquery)
# Store the project ID
projectid = "st-554"

# Set your query
sql <- "SELECT * FROM `bigquery-public-data.usa_names.usa_1910_current` LIMIT 10"

# Run the query; this returns a bq_table object that you can query further
tb <- bq_project_query(projectid, sql)

# Store the first 10 rows of the data in a tibble
sample <-bq_table_download(tb, n_max = 10)

# Print the 10 rows of data
sample
```

This code is modified from <https://bigrquery.r-dbi.org/> where they describe the `dplyr` interface for dealing with BigQuery data tables.

```{r}
library(DBI)
billing <- projectid

#set up the connection using code from the DBI section
con <- dbConnect(
  bigrquery::bigquery(),
  project = "publicdata",
  dataset = "samples",
  billing = billing
)

#see which tables (data frames) are available
dbListTables(con)

#establish a link to the natality table
natality <- tbl(con, "natality")

#pull some data
natality %>%
  select(year, month, day, weight_pounds) %>% 
  head(10) %>%
  collect()
```




